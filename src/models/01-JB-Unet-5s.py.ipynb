{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "#import os\n",
    "#import skimage.io as io\n",
    "#import skimage.transform as trans\n",
    "#import numpy as np\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "#from keras.callbacks import ModelCheckpoint, LearningRateScheduler ,EarlyStopping\n",
    "from keras import backend as keras\n",
    "from keras.metrics import *\n",
    "from keras.losses import *\n",
    "from keras import models\n",
    "#from tensorflow.keras import layers\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#custom\n",
    "import tensorflow as tf\n",
    "\n",
    "#from new tutorial\n",
    "#from tensorflow import keras\n",
    "import numpy as np\n",
    "#from tensorflow.keras.preprocessing.image import load_img\n",
    "\n",
    "'''import random\n",
    "import cv2\n",
    "import tifffile as tiff\n",
    "\n",
    "from PIL import Image ,ImageOps'''\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "#from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import math\n",
    "from datetime import datetime\n",
    "\n",
    "##beginning of others\n",
    "from tensorflow.keras.utils import Sequence , get_file\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import os\n",
    "#from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler ,EarlyStopping\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "\n",
    "%env SM_FRAMEWORK=tf.keras\n",
    "import segmentation_models as sm\n",
    "sm.set_framework('tf.keras')\n",
    "\n",
    "'''os.environ[\"KMP_BLOCKTIME\"] = \"1\"\n",
    "os.environ[\"KMP_SETTINGS\"] = \"1\"\n",
    "os.environ[\"KMP_AFFINITY\"]= \"granularity=fine,verbose,compact,1,0\"\n",
    "os.environ[\"OMP_NUM_THREADS\"]= \"32\"\n",
    "tf.config.threading.set_intra_op_parallelism_threads(32)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(2)'''\n",
    "\n",
    "\n",
    "def get_model(img_size = (512,512,3) ,num_classes=1 ,filter_sizes=[64, 128, 256]):\n",
    "    #inputs = keras.Input(shape=img_size + (3,))\n",
    "    inputs = Input(shape=img_size)\n",
    "\n",
    "    ### [First half of the network: downsampling inputs] ###\n",
    "\n",
    "    # Entry block\n",
    "    x = layers.Conv2D(filters=32, kernel_size=3, strides=2, padding=\"same\")(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    previous_block_activation = x  # Set aside residual\n",
    "\n",
    "    # Blocks 1, 2, 3 are identical apart from the feature depth.\n",
    "    for filters in filter_sizes:\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "        # Project residual\n",
    "        residual = layers.Conv2D(filters, 1, strides=2, padding=\"same\")(\n",
    "            previous_block_activation\n",
    "        )\n",
    "        x = layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    ### [Second half of the network: upsampling inputs] ###\n",
    "    filter_sizes.reverse()\n",
    "    filter_sizes.append(32)\n",
    "    \n",
    "    for filters in filter_sizes:\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.UpSampling2D(2)(x)\n",
    "\n",
    "        # Project residual\n",
    "        residual = layers.UpSampling2D(2)(previous_block_activation)\n",
    "        residual = layers.Conv2D(filters, 1, padding=\"same\")(residual)\n",
    "        x = layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    # Add a per-pixel classification layer\n",
    "    if num_classes==1:\n",
    "        outputs = layers.Conv2D(filters=num_classes, kernel_size=3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "    else:\n",
    "        outputs = layers.Conv2D(filters=num_classes, kernel_size=3, activation=\"softmax\", padding=\"same\")(x)\n",
    "\n",
    "    # Define the model\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Free up RAM in case the model definition cells were run multiple times\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# Build model\n",
    "#model = get_model((650,650,3), num_classes)\n",
    "#changes this to load only most recent h5 file, not just _e05.h5\n",
    "#model_path = \"/mnt/hgfs/VMsharedFolder/git/misc/model_checkpoint_unet3_e05.h5\"\n",
    "model_path = \"/home/hgamarro/DeepLearning/JB_space/models/Unet/model_checkpoint_unet3_e03_base.h5\"\n",
    "if os.path.isfile(model_path) & True:\n",
    "    unet = keras.models.load_model(model_path)\n",
    "    unet.summary()\n",
    "    print(\"loaded model from file\")\n",
    "else:\n",
    "    unet = get_model(num_classes=1)\n",
    "    unet.summary()\n",
    "    print(\"loaded model from code\")\n",
    "    \n",
    "    \n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, list_IDs,label_map , img_dir ,mode):\n",
    "        'Initialization'\n",
    "        self.list_IDs = list_IDs\n",
    "        self.label_map = image_label_map\n",
    "        self.on_epoch_end()\n",
    "        self.img_dir = img_dir + \"/images\"\n",
    "        self.mask_dir = img_dir + \"/masks\"\n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(len(self.list_IDs))\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        indexes = self.indexes[index:(index+1)]\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "        return X, y    \n",
    "    \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        \"\"\"Generates data containing batch_size samples\"\"\"\n",
    "        if self.mode == \"train\":\n",
    "            # Generate data\n",
    "            X, y = self.load_file(list_IDs_temp)\n",
    "            return X, y\n",
    "        elif self.mode == \"val\":\n",
    "            X, y = self.load_file(list_IDs_temp)\n",
    "            return X, y        \n",
    "        \n",
    "    def load_file(self, id_list):\n",
    "        list_IDs_temp = id_list\n",
    "        for ID in list_IDs_temp:\n",
    "            x_file_path = os.path.join(self.img_dir, ID)\n",
    "            y_file_path = os.path.join(self.mask_dir, self.label_map.get(ID))\n",
    "            # Store sample\n",
    "            X = np.load(x_file_path)\n",
    "            # Store class\n",
    "            y = np.load(y_file_path).astype('float32')\n",
    "        return X, y    \n",
    "    \n",
    "out_train_data_dir = '/home/hgamarro/DeepLearning/HG_space/data/processed/Vegas/train'\n",
    "out_val_data_dir = '/home/hgamarro/DeepLearning/HG_space/data/processed/Vegas/val'\n",
    "\n",
    "# ====================\n",
    "# train set\n",
    "# ====================\n",
    "all_files = [s for s in os.listdir(out_train_data_dir + \"/images/\") if s.endswith('.npy')]\n",
    "all_files.append([s for s in os.listdir(out_train_data_dir + \"/masks/\") if s.endswith('.npy')] )\n",
    "\n",
    "image_label_map = {\n",
    "        \"image_file_{}.npy\".format(i+1): \"label_file_{}.npy\".format(i+1)\n",
    "        for i in range(int(len(all_files)))}\n",
    "partition = [item for item in all_files if \"image_file\" in item]\n",
    "\n",
    "# ====================\n",
    "# validation set\n",
    "# ====================\n",
    "all_val_files = [s for s in os.listdir(out_val_data_dir + \"/images/\") if s.endswith('.npy')]\n",
    "all_val_files.append([s for s in os.listdir(out_val_data_dir + \"/masks/\") if s.endswith('.npy')] )\n",
    "val_image_label_map = {\n",
    "        \"image_file_{}.npy\".format(i+1): \"label_file_{}.npy\".format(i+1)\n",
    "        for i in range(int(len(all_val_files)))\n",
    "}\n",
    "val_partition = [item for item in all_val_files if \"image_file\" in item]\n",
    "\n",
    "train_generator = DataGenerator(partition,image_label_map,out_train_data_dir, \"train\")\n",
    "val_generator= DataGenerator(val_partition,val_image_label_map,out_val_data_dir, \"val\")\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.001\n",
    "EPOCHS = 15\n",
    "\n",
    "n_classes = 1  # case for binary and multiclass segmentation\n",
    "activation = 'sigmoid' if n_classes == 1 else 'softmax'\n",
    "\n",
    "# define optomizer\n",
    "optim = keras.optimizers.Adam(LR)\n",
    "\n",
    "# Segmentation models losses can be combined together by '+' and scaled by integer or float factor\n",
    "dice_loss = sm.losses.DiceLoss()\n",
    "focal_loss = sm.losses.BinaryFocalLoss() if n_classes == 1 else sm.losses.CategoricalFocalLoss()\n",
    "total_loss = dice_loss + (1 * focal_loss)\n",
    "\n",
    "# actulally total_loss can be imported directly from library, above example just show you how to manipulate with losses\n",
    "# total_loss = sm.losses.binary_focal_dice_loss # or sm.losses.categorical_focal_dice_loss \n",
    "\n",
    "metrics = [sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5)]\n",
    "\n",
    "# compile keras model with defined optimozer, loss and metrics\n",
    "unet.compile(optim, total_loss, metrics)\n",
    "\n",
    "# define callbacks for learning rate scheduling and best checkpoints saving\n",
    "\n",
    "model_andor_weight_path = \"/home/hgamarro/DeepLearning/JB_space/models/Unet/\"\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(filepath = model_andor_weight_path+'best_weights_5.h5'\n",
    "                                    ,save_freq = 'epoch'\n",
    "                                    ,verbose = 1\n",
    "                                    ,save_weights_only=True\n",
    "                                    ,save_best_only=True\n",
    "                                    ,mode='min'),\n",
    "    keras.callbacks.ReduceLROnPlateau(),\n",
    "]\n",
    "\n",
    "earlystop = EarlyStopping(monitor='accuracy'\n",
    "                         ,min_delta = .01\n",
    "                         ,patience=3)\n",
    "\n",
    "start = datetime.now()\n",
    "print(\"start: \" ,start)\n",
    " \n",
    "\n",
    "history = model.fit(\n",
    "    train_generator, \n",
    "    steps_per_epoch=len(train_generator), \n",
    "    epochs=EPOCHS, \n",
    "    callbacks=callbacks, \n",
    "    validation_data=val_generator, \n",
    "    validation_steps=len(val_generator),\n",
    "    use_multiprocessing=True\n",
    ")\n",
    "\n",
    "end = datetime.now()\n",
    "print(\"end: \" ,end)\n",
    "print(\"\\nTime Taken for testing: %s\" % (end-start))\n",
    "\n",
    "model.save(model_andor_weight_path+\"model_unet5.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
