{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Models: using `tf.keras` framework.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.utils import Sequence , get_file\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler ,EarlyStopping\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os, json, shutil\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import os\n",
    "import time\n",
    "from itertools import islice\n",
    "\n",
    "import segmentation_models as sm\n",
    "sm.set_framework('tf.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_test_data_dir = '/glade/scratch/hgamarro/Deep_learning/Final_Project//data/processed/Vegas_v1/test'\n",
    "\n",
    "model_path = \"/glade/scratch/hgamarro/Deep_learning/Final_Project/models/scratch/BD_v1_results_main/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, list_IDs,label_map , img_dir ,mode):\n",
    "        'Initialization'\n",
    "        self.list_IDs = list_IDs\n",
    "        self.label_map = label_map\n",
    "        self.on_epoch_end()\n",
    "        self.img_dir = img_dir + \"/images\"\n",
    "        self.mask_dir = img_dir + \"/masks\"\n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(len(self.list_IDs))\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))    \n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        indexes = self.indexes[index:(index+1)]\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "        return X, y\n",
    "    \n",
    "    \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        \"\"\"Generates data containing batch_size samples\"\"\"\n",
    "        if self.mode == \"train\":\n",
    "            # Generate data\n",
    "            X, y = self.load_file(list_IDs_temp)\n",
    "            return X, y\n",
    "        elif self.mode == \"val\":\n",
    "            X, y = self.load_file(list_IDs_temp)\n",
    "            return X, y\n",
    "        \n",
    "        \n",
    "    def load_file(self, id_list):\n",
    "        list_IDs_temp = id_list\n",
    "        for ID in list_IDs_temp:\n",
    "            x_file_path = os.path.join(self.img_dir, ID)\n",
    "            y_file_path = os.path.join(self.mask_dir, self.label_map.get(ID))\n",
    "            # Store sample\n",
    "            X = np.load(x_file_path)\n",
    "            # Store class\n",
    "            y = np.load(y_file_path).astype('float32')\n",
    "        return X, y    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================\n",
    "# test set\n",
    "# ====================\n",
    "\n",
    "all_test_files = [s for s in os.listdir(out_test_data_dir + \"/images/\") if s.endswith('.npy')]\n",
    "all_test_files.append([s for s in os.listdir(out_test_data_dir + \"/masks/\") if s.endswith('.npy')] )\n",
    "test_image_label_map = {\n",
    "        \"image_file_{}.npy\".format(i+1): \"label_file_{}.npy\".format(i+1)\n",
    "        for i in range(int(len(all_test_files)))}\n",
    "test_partition = [item for item in all_test_files if \"image_file\" in item]\n",
    "\n",
    "test_generator= DataGenerator(test_partition,test_image_label_map,out_test_data_dir, \"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unet\n"
     ]
    }
   ],
   "source": [
    "BACKBONE = 'vgg16'\n",
    "# BACKBONE = 'resnet50'\n",
    "# BACKBONE = 'efficientnetb6'\n",
    "model_name = 'Unet'\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.0001\n",
    "EPOCHS = 100\n",
    "\n",
    "n_classes = 1  # case for binary and multiclass segmentation\n",
    "activation = 'sigmoid' if n_classes == 1 else 'softmax'\n",
    "\n",
    "#create model\n",
    "\n",
    "\n",
    "if model_name == 'Unet':\n",
    "    print(model_name)\n",
    "    model = sm.Unet(BACKBONE,classes=n_classes,input_shape=(512, 512, 3), activation=activation)\n",
    "elif model_name == 'Linknet':\n",
    "    print(model_name)\n",
    "    model = sm.Linknet(BACKBONE,classes=n_classes,input_shape=(512, 512, 3), activation=activation)\n",
    "elif model_name == 'FPN':\n",
    "    print(model_name)\n",
    "    model = sm.FPN(BACKBONE,classes=n_classes,input_shape=(512, 512, 3), activation=activation)\n",
    "    \n",
    "\n",
    "# define optomizer\n",
    "optim = keras.optimizers.Adam(LR)\n",
    "\n",
    "# Segmentation models losses can be combined together by '+' and scaled by integer or float factor\n",
    "dice_loss = sm.losses.DiceLoss()\n",
    "focal_loss = sm.losses.BinaryFocalLoss() if n_classes == 1 else sm.losses.CategoricalFocalLoss()\n",
    "total_loss = dice_loss + (1 * focal_loss)\n",
    "\n",
    "# actulally total_loss can be imported directly from library, above example just show you how to manipulate with losses\n",
    "# total_loss = sm.losses.binary_focal_dice_loss # or sm.losses.categorical_focal_dice_loss \n",
    "\n",
    "metrics = [sm.metrics.IOUScore(threshold=0.5), sm.metrics.Precision(threshold=0.5), sm.metrics.Recall(threshold=0.5), sm.metrics.FScore(threshold=0.5)  ]\n",
    "\n",
    "# compile keras model with defined optimozer, loss and metrics\n",
    "model.compile(optim, total_loss, metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(model_path + \"vgg16_Unet.h5\")\n",
    "# model.load_weights(model_path + \"vgg16_Linknet.h5\")\n",
    "# model.load_weights(model_path + \"vgg16_FPN.h5\")\n",
    "# model.load_weights(model_path + \"efficientnetb3_Unet.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 5s 819ms/step - loss: 0.1804 - iou_score: 0.8368 - precision: 0.9225 - recall: 0.9002 - f1-score: 0.9111\n",
      "test loss, test acc: [0.18037061393260956, 0.8368191123008728, 0.9224523901939392, 0.9001668095588684, 0.9111483693122864]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test_generator)\n",
    "print(\"test loss, test acc:\", results)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
