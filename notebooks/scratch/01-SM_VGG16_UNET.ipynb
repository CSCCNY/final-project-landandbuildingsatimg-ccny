{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee6c5383-5e60-4b3d-8913-4960c1081ed2",
   "metadata": {},
   "source": [
    "# VGG-16 and Unet using one layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ca6554b-ce35-4446-9c6a-21a16d923f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r\"D:\\PhD Classes (Big_files)\\Neural Networks\\Neural Networks\\Project\\2nd_dataset_image\\Cropped_1Channel_Mask\")\n",
    "#os.chdir(r'/home/said.mejia/Projects/Deep_Learning/Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce0d3975-3fbf-4ada-8d55-30399cdaa7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import PIL # pillow - image processing\n",
    "import tensorflow as tf\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f4cc5ae-7f5b-469d-bc15-cd1bcc8bcaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_segmentation.models.unet import vgg_unet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c44a5a-422e-4fba-ac9e-ea83a767a4b6",
   "metadata": {},
   "source": [
    "# GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94a669b9-42da-4171-bcd5-d8207282183f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")    \n",
    "  except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "    \n",
    "tf.config.set_soft_device_placement(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a5116f-59f1-490b-baa4-1fc173fa54d5",
   "metadata": {},
   "source": [
    "# Changing File names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f3eab4-c609-40c6-aadb-3aedc8332db8",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd38f31-38c8-4853-84f9-76418404d8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# direc = \"Cropped_same_name\\\\test\\\\Images\\\\\"\n",
    "# reading_images = []\n",
    "# reading_masks = []\n",
    "\n",
    "# reading_images = glob.glob(r'Cropped\\test\\images\\*.png')\n",
    "# reading_masks = glob.glob(r'Cropped\\test\\masks\\*.png')\n",
    "\n",
    "# print(reading_images[0:1])\n",
    "# print(reading_masks[0:1])\n",
    "\n",
    "# print(len(reading_images))\n",
    "# print(len(reading_masks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257d82b8-af30-47de-b4d9-2b327231a1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name_images = []\n",
    "# for i, r in enumerate(reading_images):\n",
    "#     name_images.append(reading_images[i][26:])\n",
    "# print(name_images[0:1])\n",
    "# print(len(name_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7677c43-59a2-4e3e-8a8e-839c1a9fbc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, filename in enumerate(reading_images):\n",
    "#     os.rename(filename, \"Cropped_same_name\\\\test\\\\Images\\\\\" + name_images[i])\n",
    "# for i, filename in enumerate(reading_masks):\n",
    "#     os.rename(filename, \"Cropped_same_name\\\\test\\\\Masks\\\\\" + name_images[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192460da-1a41-4ffc-8760-ea5071d51b31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# name_images = reading_images[0][26:]\n",
    "# name_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94217a39-1522-4167-8c11-3a56ed25928f",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a22703b-9c40-4120-a2fc-18eb6ae97970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# direc = \"Cropped_same_name\\\\train\\\\Images\\\\\"\n",
    "# reading_images = []\n",
    "# reading_masks = []\n",
    "\n",
    "# reading_images = glob.glob(r'Cropped\\train\\images\\*.png')\n",
    "# reading_masks = glob.glob(r'Cropped\\train\\masks\\*.png')\n",
    "\n",
    "# print(reading_images[0:1])\n",
    "# print(reading_masks[0:1])\n",
    "\n",
    "# print(len(reading_images))\n",
    "# print(len(reading_masks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed500f5-794c-443d-b977-28dc00293387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name_images = []\n",
    "# for i, r in enumerate(reading_images):\n",
    "#     name_images.append(reading_images[i][27:])\n",
    "# print(name_images[0:1])\n",
    "# print(len(name_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1600315f-b95c-43d3-b00e-3299c30018f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, filename in enumerate(reading_images):\n",
    "#     os.rename(filename, \"Cropped_same_name\\\\train\\\\Images\\\\\" + name_images[i])\n",
    "# for i, filename in enumerate(reading_masks):\n",
    "#     os.rename(filename, \"Cropped_same_name\\\\train\\\\Masks\\\\\" + name_images[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4645c5-9024-4883-9cc2-8576d550a068",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f999a6d9-78a0-4a64-b8b3-a4226d921c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# direc = \"Cropped_same_name\\\\val\\\\Images\\\\\"\n",
    "# reading_images = []\n",
    "# reading_masks = []\n",
    "\n",
    "# reading_images = glob.glob(r'Cropped\\val\\images\\*.png')\n",
    "# reading_masks = glob.glob(r'Cropped\\val\\masks\\*.png')\n",
    "\n",
    "# print(reading_images[0:1])\n",
    "# print(reading_masks[0:1])\n",
    "\n",
    "# print(len(reading_images))\n",
    "# print(len(reading_masks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544ba497-7541-4ad5-bd5a-388f249ec06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name_images = []\n",
    "# for i, r in enumerate(reading_images):\n",
    "#     name_images.append(reading_images[i][25:])\n",
    "# print(name_images[0:1])\n",
    "# print(len(name_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f777895-9bfd-4686-8ded-5a9b41932ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, filename in enumerate(reading_images):\n",
    "#     os.rename(filename, \"Cropped_same_name\\\\val\\\\Images\\\\\" + name_images[i])\n",
    "# for i, filename in enumerate(reading_masks):\n",
    "#     os.rename(filename, \"Cropped_same_name\\\\val\\\\Masks\\\\\" + name_images[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b501fc-6bd3-4cc6-9056-446becbc0626",
   "metadata": {},
   "source": [
    "# Cropping Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119f2d86-99de-4da9-ae2e-eb92088b1846",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c9c49bb4-ad76-46af-b60d-2c272ee1df45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cropped_same_name\\test\\Images\\Batch_0_Image_11_Tile_8.png\n",
      "['Cropped_same_name\\\\test\\\\Masks\\\\Batch_0_Image_0_Tile_5.png']\n"
     ]
    }
   ],
   "source": [
    "reading_images = glob.glob(r'Cropped_same_name\\test\\Images\\*.png')\n",
    "reading_masks = glob.glob(r'Cropped_same_name\\test\\Masks\\*.png')\n",
    "\n",
    "print(reading_images[5])\n",
    "print(reading_masks[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0e7920-dadb-49cf-a426-e127b3d7f248",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in reading_images:\n",
    "    im_image = Image.open(i)\n",
    "    # im_image.show()\n",
    "    cropped_image =  im_image.crop((0,0,511,511))\n",
    "    # cropped_image.show()\n",
    "    cropped_image.save(i.replace('Cropped_same_name','Cropped_same_name_cropped'))\n",
    "    \n",
    "for i in reading_masks:\n",
    "    im_mask = Image.open(i)\n",
    "    # im_image.show()\n",
    "    cropped_mask =  im_mask.crop((0,0,511,511))\n",
    "    # cropped_image.show()\n",
    "    cropped_mask.save(i.replace('Cropped_same_name','Cropped_same_name_cropped'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec94c840-e16b-4e75-994f-56134c1bb103",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304a703b-1d39-4a25-a186-d461939ca7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "reading_images = glob.glob(r'Cropped_same_name\\train\\Images\\*.png')\n",
    "reading_masks = glob.glob(r'Cropped_same_name\\train\\Masks\\*.png')\n",
    "\n",
    "print(reading_images[5])\n",
    "print(reading_masks[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e48d27b-4ba5-45cf-b627-dc024276f545",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in reading_images:\n",
    "    im_image = Image.open(i)\n",
    "    # im_image.show()\n",
    "    cropped_image =  im_image.crop((0,0,511,511))\n",
    "    # cropped_image.show()\n",
    "    cropped_image.save(i.replace('Cropped_same_name','Cropped_same_name_cropped'))\n",
    "    \n",
    "for i in reading_masks:\n",
    "    im_mask = Image.open(i)\n",
    "    # im_image.show()\n",
    "    cropped_mask =  im_mask.crop((0,0,511,511))\n",
    "    # cropped_image.show()\n",
    "    cropped_mask.save(i.replace('Cropped_same_name','Cropped_same_name_cropped'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdb41b5-fe76-4d9c-8ba6-4123e64801eb",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372a412f-a694-4159-bde1-993e0a8b945c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reading_images = glob.glob(r'Cropped_same_name\\val\\Images\\*.png')\n",
    "reading_masks = glob.glob(r'Cropped_same_name\\val\\Masks\\*.png')\n",
    "\n",
    "print(reading_images[5])\n",
    "print(reading_masks[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fadc58-2e71-4609-b84b-a87cda8147b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in reading_images:\n",
    "    im_image = Image.open(i)\n",
    "    # im_image.show()\n",
    "    cropped_image =  im_image.crop((0,0,511,511))\n",
    "    # cropped_image.show()\n",
    "    cropped_image.save(i.replace('Cropped_same_name','Cropped_same_name_cropped'))\n",
    "    \n",
    "for i in reading_masks:\n",
    "    im_mask = Image.open(i)\n",
    "    # im_image.show()\n",
    "    cropped_mask =  im_mask.crop((0,0,511,511))\n",
    "    # cropped_image.show()\n",
    "    cropped_mask.save(i.replace('Cropped_same_name','Cropped_same_name_cropped'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c031b156-c775-43bb-89d4-06a643d48fef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4049eff-b756-4a5c-83a5-a7d674344374",
   "metadata": {},
   "source": [
    "# Reading an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74169a5-5db1-4660-b3ad-496fdc88b5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "reading_images = glob.glob(r'Cropped_same_name\\train\\Images\\*.png')\n",
    "reading_images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938fdd68-aa64-4d7a-8e7e-c6e2ecf7fb68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = PIL.Image.open(reading_images[0], 'r')\n",
    "plt.imshow(np.asarray(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0ecbf4-db5f-4964-b86d-30b0a7e8b2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_array =  np.asarray(img)\n",
    "image_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf64eb1-f8e3-44b1-aaba-3cebc5a5f6e0",
   "metadata": {},
   "source": [
    "# Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd8c134-d11e-497b-9eb5-3a13f034e065",
   "metadata": {},
   "source": [
    "## ? Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254e06cc-33c3-4681-9a21-7148e5d75c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import random\n",
    "import six\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89eac7f-765e-4e78-abd0-cc39698b5302",
   "metadata": {},
   "source": [
    "## 6 Image_segmentation_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f7506a-b3d5-42e4-87e1-5b462d1542e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import six\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def image_segmentation_generator(images_path, segs_path, batch_size,\n",
    "                                 n_classes, input_height, input_width,\n",
    "                                 output_height, output_width,\n",
    "                                 do_augment=False,\n",
    "                                 augmentation_name=\"aug_all\",\n",
    "                                 custom_augmentation=None,\n",
    "                                 other_inputs_paths=None, preprocessing=None,\n",
    "                                 read_image_type=cv2.IMREAD_COLOR , ignore_segs=False ):\n",
    "    \n",
    "\n",
    "    if not ignore_segs:\n",
    "        img_seg_pairs = get_pairs_from_paths(images_path, segs_path, other_inputs_paths=other_inputs_paths)\n",
    "        random.shuffle(img_seg_pairs)\n",
    "        zipped = itertools.cycle(img_seg_pairs)\n",
    "    else:\n",
    "        img_list = get_image_list_from_path( images_path )\n",
    "        random.shuffle( img_list )\n",
    "        img_list_gen = itertools.cycle( img_list )\n",
    "\n",
    "\n",
    "    while True:\n",
    "        X = []\n",
    "        Y = []\n",
    "        for _ in range(batch_size):\n",
    "            if other_inputs_paths is None:\n",
    "\n",
    "                if ignore_segs:\n",
    "                    im = next( img_list_gen )\n",
    "                    seg = None \n",
    "                else:\n",
    "                    im, seg = next(zipped)\n",
    "                    seg = cv2.imread(seg, 1)\n",
    "\n",
    "                im = cv2.imread(im, read_image_type)\n",
    "                \n",
    "\n",
    "                if do_augment:\n",
    "\n",
    "                    assert ignore_segs == False , \"Not supported yet\"\n",
    "\n",
    "                    if custom_augmentation is None:\n",
    "                        im, seg[:, :, 0] = augment_seg(im, seg[:, :, 0],\n",
    "                                                       augmentation_name)\n",
    "                    else:\n",
    "                        im, seg[:, :, 0] = custom_augment_seg(im, seg[:, :, 0],\n",
    "                                                              custom_augmentation)\n",
    "\n",
    "                if preprocessing is not None:\n",
    "                    im = preprocessing(im)\n",
    "\n",
    "                X.append(get_image_array(im, input_width,\n",
    "                                         input_height, ordering=IMAGE_ORDERING))\n",
    "            else:\n",
    "\n",
    "                assert ignore_segs == False , \"Not supported yet\"\n",
    "\n",
    "                im, seg, others = next(zipped)\n",
    "\n",
    "                im = cv2.imread(im, read_image_type)\n",
    "                seg = cv2.imread(seg, 1)\n",
    "\n",
    "                oth = []\n",
    "                for f in others:\n",
    "                    oth.append(cv2.imread(f, read_image_type))\n",
    "\n",
    "                if do_augment:\n",
    "                    if custom_augmentation is None:\n",
    "                        ims, seg[:, :, 0] = augment_seg(im, seg[:, :, 0],\n",
    "                                                        augmentation_name, other_imgs=oth)\n",
    "                    else:\n",
    "                        ims, seg[:, :, 0] = custom_augment_seg(im, seg[:, :, 0],\n",
    "                                                               custom_augmentation, other_imgs=oth)\n",
    "                else:\n",
    "                    ims = [im]\n",
    "                    ims.extend(oth)\n",
    "\n",
    "                oth = []\n",
    "                for i, image in enumerate(ims):\n",
    "                    oth_im = get_image_array(image, input_width,\n",
    "                                             input_height, ordering=IMAGE_ORDERING)\n",
    "\n",
    "                    if preprocessing is not None:\n",
    "                        if isinstance(preprocessing, Sequence):\n",
    "                            oth_im = preprocessing[i](oth_im)\n",
    "                        else:\n",
    "                            oth_im = preprocessing(oth_im)\n",
    "\n",
    "                    oth.append(oth_im)\n",
    "\n",
    "                X.append(oth)\n",
    "\n",
    "            if not ignore_segs:\n",
    "                Y.append(get_segmentation_array(\n",
    "                    seg, n_classes, output_width, output_height))\n",
    "\n",
    "        if ignore_segs:\n",
    "            yield np.array(X)\n",
    "        else:\n",
    "            yield np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fda61f-c893-4062-bcfe-1c05a2893ba1",
   "metadata": {},
   "source": [
    "## 7 verify_segmentation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b18ef1f-946e-4241-83a5-7b0d91ff73e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_segmentation_dataset(images_path, segs_path,\n",
    "                                n_classes, show_all_errors=False):\n",
    "    try:\n",
    "        img_seg_pairs = get_pairs_from_paths(images_path, segs_path)\n",
    "        if not len(img_seg_pairs):\n",
    "            print(\"Couldn't load any data from images_path: \"\n",
    "                  \"{0} and segmentations path: {1}\"\n",
    "                  .format(images_path, segs_path))\n",
    "            return False\n",
    "\n",
    "        return_value = True\n",
    "        for im_fn, seg_fn in tqdm(img_seg_pairs):\n",
    "            img = cv2.imread(im_fn)\n",
    "            seg = cv2.imread(seg_fn)\n",
    "            # Check dimensions match\n",
    "            if not img.shape == seg.shape:\n",
    "                return_value = False\n",
    "                print(\"The size of image {0} and its segmentation {1} \"\n",
    "                      \"doesn't match (possibly the files are corrupt).\"\n",
    "                      .format(im_fn, seg_fn))\n",
    "                if not show_all_errors:\n",
    "                    break\n",
    "            else:\n",
    "                max_pixel_value = np.max(seg[:, :, 0])\n",
    "                if max_pixel_value >= n_classes:\n",
    "                    return_value = False\n",
    "                    print(\"The pixel values of the segmentation image {0} \"\n",
    "                          \"violating range [0, {1}]. \"\n",
    "                          \"Found maximum pixel value {2}\"\n",
    "                          .format(seg_fn, str(n_classes - 1), max_pixel_value))\n",
    "                    if not show_all_errors:\n",
    "                        break\n",
    "        if return_value:\n",
    "            print(\"Dataset verified! \")\n",
    "        else:\n",
    "            print(\"Dataset not verified!\")\n",
    "        return return_value\n",
    "    except DataLoaderError as e:\n",
    "        print(\"Found error during data loading\\n{0}\".format(str(e)))\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0bc49e-b9d6-4cdb-bd42-a4dae2d5fe68",
   "metadata": {},
   "source": [
    "## 5 Model Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf58c3f-1826-45c1-abd0-3d773f2ff4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import six\n",
    "from keras.callbacks import Callback\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b9899d-ee99-41e7-b36b-5cf5915cc459",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_from_name = {}\n",
    "\n",
    "\n",
    "# model_from_name[\"fcn_8\"] = fcn.fcn_8\n",
    "# model_from_name[\"fcn_32\"] = fcn.fcn_32\n",
    "# model_from_name[\"fcn_8_vgg\"] = fcn.fcn_8_vgg\n",
    "# model_from_name[\"fcn_32_vgg\"] = fcn.fcn_32_vgg\n",
    "# model_from_name[\"fcn_8_resnet50\"] = fcn.fcn_8_resnet50\n",
    "# model_from_name[\"fcn_32_resnet50\"] = fcn.fcn_32_resnet50\n",
    "# model_from_name[\"fcn_8_mobilenet\"] = fcn.fcn_8_mobilenet\n",
    "# model_from_name[\"fcn_32_mobilenet\"] = fcn.fcn_32_mobilenet\n",
    "\n",
    "\n",
    "# model_from_name[\"pspnet\"] = pspnet.pspnet\n",
    "# model_from_name[\"vgg_pspnet\"] = pspnet.vgg_pspnet\n",
    "# model_from_name[\"resnet50_pspnet\"] = pspnet.resnet50_pspnet\n",
    "\n",
    "# model_from_name[\"vgg_pspnet\"] = pspnet.vgg_pspnet\n",
    "# model_from_name[\"resnet50_pspnet\"] = pspnet.resnet50_pspnet\n",
    "\n",
    "# model_from_name[\"pspnet_50\"] = pspnet.pspnet_50\n",
    "# model_from_name[\"pspnet_101\"] = pspnet.pspnet_101\n",
    "\n",
    "\n",
    "# model_from_name[\"mobilenet_pspnet\"] = pspnet.mobilenet_pspnet\n",
    "\n",
    "\n",
    "# model_from_name[\"unet_mini\"] = unet.unet_mini\n",
    "# model_from_name[\"unet\"] = unet.unet\n",
    "model_from_name[\"vgg_unet\"] = vgg_unet # original unet.vgg_unet\n",
    "# model_from_name[\"resnet50_unet\"] = resnet50_unet\n",
    "# model_from_name[\"mobilenet_unet\"] = mobilenet_unet\n",
    "\n",
    "\n",
    "# model_from_name[\"segnet\"] = segnet.segnet\n",
    "# model_from_name[\"vgg_segnet\"] = segnet.vgg_segnet\n",
    "# model_from_name[\"resnet50_segnet\"] = segnet.resnet50_segnet\n",
    "# model_from_name[\"mobilenet_segnet\"] = segnet.mobilenet_segnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62fd8db-6909-488c-b3eb-56c739f31820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# from .data_utils.data_loader import image_segmentation_generator, \\\n",
    "#     verify_segmentation_dataset\n",
    "import six\n",
    "from keras.callbacks import Callback\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "def find_latest_checkpoint(checkpoints_path, fail_safe=True):\n",
    "\n",
    "    # This is legacy code, there should always be a \"checkpoint\" file in your directory\n",
    "\n",
    "    def get_epoch_number_from_path(path):\n",
    "        return path.replace(checkpoints_path, \"\").strip(\".\")\n",
    "\n",
    "    # Get all matching files\n",
    "    all_checkpoint_files = glob.glob(checkpoints_path + \".*\")\n",
    "    if len(all_checkpoint_files) == 0:\n",
    "        all_checkpoint_files = glob.glob(checkpoints_path + \"*.*\")\n",
    "    all_checkpoint_files = [ff.replace(\".index\", \"\") for ff in\n",
    "                            all_checkpoint_files]  # to make it work for newer versions of keras\n",
    "    # Filter out entries where the epoc_number part is pure number\n",
    "    all_checkpoint_files = list(filter(lambda f: get_epoch_number_from_path(f)\n",
    "                                       .isdigit(), all_checkpoint_files))\n",
    "    if not len(all_checkpoint_files):\n",
    "        # The glob list is empty, don't have a checkpoints_path\n",
    "        if not fail_safe:\n",
    "            raise ValueError(\"Checkpoint path {0} invalid\"\n",
    "                             .format(checkpoints_path))\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    # Find the checkpoint file with the maximum epoch\n",
    "    latest_epoch_checkpoint = max(all_checkpoint_files,\n",
    "                                  key=lambda f:\n",
    "                                  int(get_epoch_number_from_path(f)))\n",
    "\n",
    "    return latest_epoch_checkpoint\n",
    "\n",
    "def masked_categorical_crossentropy(gt, pr):\n",
    "    from keras.losses import categorical_crossentropy\n",
    "    mask = 1 - gt[:, :, 0]\n",
    "    return categorical_crossentropy(gt, pr) * mask\n",
    "\n",
    "\n",
    "class CheckpointsCallback(Callback):\n",
    "    def __init__(self, checkpoints_path):\n",
    "        self.checkpoints_path = checkpoints_path\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.checkpoints_path is not None:\n",
    "            self.model.save_weights(self.checkpoints_path + \".\" + str(epoch))\n",
    "            print(\"saved \", self.checkpoints_path + \".\" + str(epoch))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a607de-ba57-43be-a7d3-7a37edf96777",
   "metadata": {},
   "source": [
    "## 4 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f69ba06-4335-4696-badb-f0be0b5a63b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,\n",
    "          train_images,\n",
    "          train_annotations,\n",
    "          input_height=None,\n",
    "          input_width=None,\n",
    "          n_classes=None,\n",
    "          verify_dataset=True,\n",
    "          checkpoints_path=None,\n",
    "          epochs=5,\n",
    "          batch_size=2,\n",
    "          validate=False,\n",
    "          val_images=None,\n",
    "          val_annotations=None,\n",
    "          val_batch_size=2,\n",
    "          auto_resume_checkpoint=False,\n",
    "          load_weights=None,\n",
    "          steps_per_epoch=512,\n",
    "          val_steps_per_epoch=512,\n",
    "          gen_use_multiprocessing=False,\n",
    "          ignore_zero_class=False,\n",
    "          optimizer_name='adam',\n",
    "          do_augment=False,\n",
    "          augmentation_name=\"aug_all\",\n",
    "          callbacks=None,\n",
    "          custom_augmentation=None,\n",
    "          other_inputs_paths=None,\n",
    "          preprocessing=None,\n",
    "          read_image_type=1  # cv2.IMREAD_COLOR = 1 (rgb),\n",
    "                             # cv2.IMREAD_GRAYSCALE = 0,\n",
    "                             # cv2.IMREAD_UNCHANGED = -1 (4 channels like RGBA)\n",
    "         ):\n",
    "#     from .models.all_models import model_from_name\n",
    "\n",
    "    # check if user gives model name instead of the model object\n",
    "    if isinstance(model, six.string_types):\n",
    "        # create the model from the name\n",
    "        assert (n_classes is not None), \"Please provide the n_classes\"\n",
    "        if (input_height is not None) and (input_width is not None):\n",
    "            model = model_from_name[model](\n",
    "                n_classes, input_height=input_height, input_width=input_width)\n",
    "        else:\n",
    "            model = model_from_name[model](n_classes)\n",
    "\n",
    "    n_classes = model.n_classes\n",
    "    input_height = model.input_height\n",
    "    input_width = model.input_width\n",
    "    output_height = model.output_height\n",
    "    output_width = model.output_width\n",
    "\n",
    "    if validate:\n",
    "        assert val_images is not None\n",
    "        assert val_annotations is not None\n",
    "\n",
    "    if optimizer_name is not None:\n",
    "\n",
    "        if ignore_zero_class:\n",
    "            loss_k = masked_categorical_crossentropy\n",
    "        else:\n",
    "            loss_k = 'categorical_crossentropy'\n",
    "\n",
    "        model.compile(loss=loss_k,\n",
    "                      optimizer=optimizer_name,\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "    if checkpoints_path is not None:\n",
    "        config_file = checkpoints_path + \"_config.json\"\n",
    "        dir_name = os.path.dirname(config_file)\n",
    "\n",
    "        if ( not os.path.exists(dir_name) )  and len( dir_name ) > 0 :\n",
    "            os.makedirs(dir_name)\n",
    "\n",
    "        with open(config_file, \"w\") as f:\n",
    "            json.dump({\n",
    "                \"model_class\": model.model_name,\n",
    "                \"n_classes\": n_classes,\n",
    "                \"input_height\": input_height,\n",
    "                \"input_width\": input_width,\n",
    "                \"output_height\": output_height,\n",
    "                \"output_width\": output_width\n",
    "            }, f)\n",
    "\n",
    "    if load_weights is not None and len(load_weights) > 0:\n",
    "        print(\"Loading weights from \", load_weights)\n",
    "        model.load_weights(load_weights)\n",
    "\n",
    "    initial_epoch = 0\n",
    "\n",
    "    if auto_resume_checkpoint and (checkpoints_path is not None):\n",
    "        latest_checkpoint = find_latest_checkpoint(checkpoints_path)\n",
    "        if latest_checkpoint is not None:\n",
    "            print(\"Loading the weights from latest checkpoint \",\n",
    "                  latest_checkpoint)\n",
    "            model.load_weights(latest_checkpoint)\n",
    "\n",
    "            initial_epoch = int(latest_checkpoint.split('.')[-1])\n",
    "\n",
    "    if verify_dataset:\n",
    "        print(\"Verifying training dataset\")\n",
    "        verified = verify_segmentation_dataset(train_images,\n",
    "                                               train_annotations,\n",
    "                                               n_classes)\n",
    "        assert verified\n",
    "        if validate:\n",
    "            print(\"Verifying validation dataset\")\n",
    "            verified = verify_segmentation_dataset(val_images,\n",
    "                                                   val_annotations,\n",
    "                                                   n_classes)\n",
    "            assert verified\n",
    "\n",
    "    train_gen = image_segmentation_generator(\n",
    "        train_images, train_annotations,  batch_size,  n_classes,\n",
    "        input_height, input_width, output_height, output_width,\n",
    "        do_augment=do_augment, augmentation_name=augmentation_name,\n",
    "        custom_augmentation=custom_augmentation, other_inputs_paths=other_inputs_paths,\n",
    "        preprocessing=preprocessing, read_image_type=read_image_type)\n",
    "\n",
    "    if validate:\n",
    "        val_gen = image_segmentation_generator(\n",
    "            val_images, val_annotations,  val_batch_size,\n",
    "            n_classes, input_height, input_width, output_height, output_width,\n",
    "            other_inputs_paths=other_inputs_paths,\n",
    "            preprocessing=preprocessing, read_image_type=read_image_type)\n",
    "\n",
    "    if callbacks is None and (not checkpoints_path is  None) :\n",
    "        default_callback = ModelCheckpoint(\n",
    "                filepath=checkpoints_path + \".{epoch:05d}\",\n",
    "                save_weights_only=True,\n",
    "                verbose=True\n",
    "            )\n",
    "\n",
    "        if sys.version_info[0] < 3: # for pyhton 2 \n",
    "            default_callback = CheckpointsCallback(checkpoints_path)\n",
    "\n",
    "        callbacks = [\n",
    "            default_callback\n",
    "        ]\n",
    "\n",
    "    if callbacks is None:\n",
    "        callbacks = []\n",
    "\n",
    "    if not validate:\n",
    "        model.fit(train_gen, steps_per_epoch=steps_per_epoch,\n",
    "                  epochs=epochs, callbacks=callbacks, initial_epoch=initial_epoch)\n",
    "    else:\n",
    "        model.fit(train_gen,\n",
    "                  steps_per_epoch=steps_per_epoch,\n",
    "                  validation_data=val_gen,\n",
    "                  validation_steps=val_steps_per_epoch,\n",
    "                  epochs=epochs, callbacks=callbacks,\n",
    "                  use_multiprocessing=gen_use_multiprocessing, initial_epoch=initial_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7ccef4-43f1-4303-b44a-efa2c49bc8f9",
   "metadata": {},
   "source": [
    "## ? Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3932aad7-e66b-4741-aa38-41d5e3443c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "import six\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0dfbf1-e353-46da-996e-8794b638be7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_from_checkpoint_path(checkpoints_path):\n",
    "\n",
    "    from .models.all_models import model_from_name\n",
    "    assert (os.path.isfile(checkpoints_path+\"_config.json\")\n",
    "            ), \"Checkpoint not found.\"\n",
    "    model_config = json.loads(\n",
    "        open(checkpoints_path+\"_config.json\", \"r\").read())\n",
    "    latest_weights = find_latest_checkpoint(checkpoints_path)\n",
    "    assert (latest_weights is not None), \"Checkpoint not found.\"\n",
    "    model = model_from_name[model_config['model_class']](\n",
    "        model_config['n_classes'], input_height=model_config['input_height'],\n",
    "        input_width=model_config['input_width'])\n",
    "    print(\"loaded weights \", latest_weights)\n",
    "    status = model.load_weights(latest_weights)\n",
    "\n",
    "    if status is not None:\n",
    "        status.expect_partial()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_colored_segmentation_image(seg_arr, n_classes, colors=class_colors):\n",
    "    output_height = seg_arr.shape[0]\n",
    "    output_width = seg_arr.shape[1]\n",
    "\n",
    "    seg_img = np.zeros((output_height, output_width, 3))\n",
    "\n",
    "    for c in range(n_classes):\n",
    "        seg_arr_c = seg_arr[:, :] == c\n",
    "        seg_img[:, :, 0] += ((seg_arr_c)*(colors[c][0])).astype('uint8')\n",
    "        seg_img[:, :, 1] += ((seg_arr_c)*(colors[c][1])).astype('uint8')\n",
    "        seg_img[:, :, 2] += ((seg_arr_c)*(colors[c][2])).astype('uint8')\n",
    "\n",
    "    return seg_img\n",
    "\n",
    "\n",
    "def get_legends(class_names, colors=class_colors):\n",
    "\n",
    "    n_classes = len(class_names)\n",
    "    legend = np.zeros(((len(class_names) * 25) + 25, 125, 3),\n",
    "                      dtype=\"uint8\") + 255\n",
    "\n",
    "    class_names_colors = enumerate(zip(class_names[:n_classes],\n",
    "                                       colors[:n_classes]))\n",
    "\n",
    "    for (i, (class_name, color)) in class_names_colors:\n",
    "        color = [int(c) for c in color]\n",
    "        cv2.putText(legend, class_name, (5, (i * 25) + 17),\n",
    "                    cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 0), 1)\n",
    "        cv2.rectangle(legend, (100, (i * 25)), (125, (i * 25) + 25),\n",
    "                      tuple(color), -1)\n",
    "\n",
    "    return legend\n",
    "\n",
    "\n",
    "def overlay_seg_image(inp_img, seg_img):\n",
    "    orininal_h = inp_img.shape[0]\n",
    "    orininal_w = inp_img.shape[1]\n",
    "    seg_img = cv2.resize(seg_img, (orininal_w, orininal_h), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    fused_img = (inp_img/2 + seg_img/2).astype('uint8')\n",
    "    return fused_img\n",
    "\n",
    "\n",
    "def concat_lenends(seg_img, legend_img):\n",
    "\n",
    "    new_h = np.maximum(seg_img.shape[0], legend_img.shape[0])\n",
    "    new_w = seg_img.shape[1] + legend_img.shape[1]\n",
    "\n",
    "    out_img = np.zeros((new_h, new_w, 3)).astype('uint8') + legend_img[0, 0, 0]\n",
    "\n",
    "    out_img[:legend_img.shape[0], :  legend_img.shape[1]] = np.copy(legend_img)\n",
    "    out_img[:seg_img.shape[0], legend_img.shape[1]:] = np.copy(seg_img)\n",
    "\n",
    "    return out_img\n",
    "\n",
    "\n",
    "def visualize_segmentation(seg_arr, inp_img=None, n_classes=None,\n",
    "                           colors=class_colors, class_names=None,\n",
    "                           overlay_img=False, show_legends=False,\n",
    "                           prediction_width=None, prediction_height=None):\n",
    "\n",
    "    if n_classes is None:\n",
    "        n_classes = np.max(seg_arr)\n",
    "\n",
    "    seg_img = get_colored_segmentation_image(seg_arr, n_classes, colors=colors)\n",
    "\n",
    "    if inp_img is not None:\n",
    "        original_h = inp_img.shape[0]\n",
    "        original_w = inp_img.shape[1]\n",
    "        seg_img = cv2.resize(seg_img, (original_w, original_h), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    if (prediction_height is not None) and (prediction_width is not None):\n",
    "        seg_img = cv2.resize(seg_img, (prediction_width, prediction_height), interpolation=cv2.INTER_NEAREST)\n",
    "        if inp_img is not None:\n",
    "            inp_img = cv2.resize(inp_img,\n",
    "                                 (prediction_width, prediction_height))\n",
    "\n",
    "    if overlay_img:\n",
    "        assert inp_img is not None\n",
    "        seg_img = overlay_seg_image(inp_img, seg_img)\n",
    "\n",
    "    if show_legends:\n",
    "        assert class_names is not None\n",
    "        legend_img = get_legends(class_names, colors=colors)\n",
    "\n",
    "        seg_img = concat_lenends(seg_img, legend_img)\n",
    "\n",
    "    return seg_img\n",
    "\n",
    "\n",
    "def predict(model=None, inp=None, out_fname=None,\n",
    "            checkpoints_path=None, overlay_img=False,\n",
    "            class_names=None, show_legends=False, colors=class_colors,\n",
    "            prediction_width=None, prediction_height=None,\n",
    "            read_image_type=1):\n",
    "\n",
    "    if model is None and (checkpoints_path is not None):\n",
    "        model = model_from_checkpoint_path(checkpoints_path)\n",
    "\n",
    "    assert (inp is not None)\n",
    "    assert ((type(inp) is np.ndarray) or isinstance(inp, six.string_types)),\\\n",
    "        \"Input should be the CV image or the input file name\"\n",
    "\n",
    "    if isinstance(inp, six.string_types):\n",
    "        inp = cv2.imread(inp, read_image_type)\n",
    "\n",
    "    assert (len(inp.shape) == 3 or len(inp.shape) == 1 or len(inp.shape) == 4), \"Image should be h,w,3 \"\n",
    "\n",
    "    output_width = model.output_width\n",
    "    output_height = model.output_height\n",
    "    input_width = model.input_width\n",
    "    input_height = model.input_height\n",
    "    n_classes = model.n_classes\n",
    "\n",
    "    x = get_image_array(inp, input_width, input_height,\n",
    "                        ordering=IMAGE_ORDERING)\n",
    "    pr = model.predict(np.array([x]))[0]\n",
    "    pr = pr.reshape((output_height,  output_width, n_classes)).argmax(axis=2)\n",
    "\n",
    "    seg_img = visualize_segmentation(pr, inp, n_classes=n_classes,\n",
    "                                     colors=colors, overlay_img=overlay_img,\n",
    "                                     show_legends=show_legends,\n",
    "                                     class_names=class_names,\n",
    "                                     prediction_width=prediction_width,\n",
    "                                     prediction_height=prediction_height)\n",
    "\n",
    "    if out_fname is not None:\n",
    "        cv2.imwrite(out_fname, seg_img)\n",
    "\n",
    "    return pr\n",
    "\n",
    "\n",
    "def predict_multiple(model=None, inps=None, inp_dir=None, out_dir=None,\n",
    "                     checkpoints_path=None, overlay_img=False,\n",
    "                     class_names=None, show_legends=False, colors=class_colors,\n",
    "                     prediction_width=None, prediction_height=None, read_image_type=1):\n",
    "\n",
    "    if model is None and (checkpoints_path is not None):\n",
    "        model = model_from_checkpoint_path(checkpoints_path)\n",
    "\n",
    "    if inps is None and (inp_dir is not None):\n",
    "        inps = glob.glob(os.path.join(inp_dir, \"*.jpg\")) + glob.glob(\n",
    "            os.path.join(inp_dir, \"*.png\")) + \\\n",
    "            glob.glob(os.path.join(inp_dir, \"*.jpeg\"))\n",
    "        inps = sorted(inps)\n",
    "\n",
    "    assert type(inps) is list\n",
    "\n",
    "    all_prs = []\n",
    "\n",
    "    if not out_dir is None:\n",
    "        if not os.path.exists(out_dir):\n",
    "            os.makedirs(out_dir)\n",
    "\n",
    "\n",
    "    for i, inp in enumerate(tqdm(inps)):\n",
    "        if out_dir is None:\n",
    "            out_fname = None\n",
    "        else:\n",
    "            if isinstance(inp, six.string_types):\n",
    "                out_fname = os.path.join(out_dir, os.path.basename(inp))\n",
    "            else:\n",
    "                out_fname = os.path.join(out_dir, str(i) + \".jpg\")\n",
    "\n",
    "        pr = predict(model, inp, out_fname,\n",
    "                     overlay_img=overlay_img, class_names=class_names,\n",
    "                     show_legends=show_legends, colors=colors,\n",
    "                     prediction_width=prediction_width,\n",
    "                     prediction_height=prediction_height, read_image_type=read_image_type)\n",
    "\n",
    "        all_prs.append(pr)\n",
    "\n",
    "    return all_prs\n",
    "\n",
    "\n",
    "def set_video(inp, video_name):\n",
    "    cap = cv2.VideoCapture(inp)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    video_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    video_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    size = (video_width, video_height)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"XVID\")\n",
    "    video = cv2.VideoWriter(video_name, fourcc, fps, size)\n",
    "    return cap, video, fps\n",
    "\n",
    "\n",
    "def predict_video(model=None, inp=None, output=None,\n",
    "                  checkpoints_path=None, display=False, overlay_img=True,\n",
    "                  class_names=None, show_legends=False, colors=class_colors,\n",
    "                  prediction_width=None, prediction_height=None):\n",
    "\n",
    "    if model is None and (checkpoints_path is not None):\n",
    "        model = model_from_checkpoint_path(checkpoints_path)\n",
    "    n_classes = model.n_classes\n",
    "\n",
    "    cap, video, fps = set_video(inp, output)\n",
    "    while(cap.isOpened()):\n",
    "        prev_time = time()\n",
    "        ret, frame = cap.read()\n",
    "        if frame is not None:\n",
    "            pr = predict(model=model, inp=frame)\n",
    "            fused_img = visualize_segmentation(\n",
    "                pr, frame, n_classes=n_classes,\n",
    "                colors=colors,\n",
    "                overlay_img=overlay_img,\n",
    "                show_legends=show_legends,\n",
    "                class_names=class_names,\n",
    "                prediction_width=prediction_width,\n",
    "                prediction_height=prediction_height\n",
    "                )\n",
    "        else:\n",
    "            break\n",
    "        print(\"FPS: {}\".format(1/(time() - prev_time)))\n",
    "        if output is not None:\n",
    "            video.write(fused_img)\n",
    "        if display:\n",
    "            cv2.imshow('Frame masked', fused_img)\n",
    "            if cv2.waitKey(fps) & 0xFF == ord('q'):\n",
    "                break\n",
    "    cap.release()\n",
    "    if output is not None:\n",
    "        video.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "def evaluate(model=None, inp_images=None, annotations=None,\n",
    "             inp_images_dir=None, annotations_dir=None, checkpoints_path=None, read_image_type=1):\n",
    "\n",
    "    if model is None:\n",
    "        assert (checkpoints_path is not None),\\\n",
    "                \"Please provide the model or the checkpoints_path\"\n",
    "        model = model_from_checkpoint_path(checkpoints_path)\n",
    "\n",
    "    if inp_images is None:\n",
    "        assert (inp_images_dir is not None),\\\n",
    "                \"Please provide inp_images or inp_images_dir\"\n",
    "        assert (annotations_dir is not None),\\\n",
    "            \"Please provide inp_images or inp_images_dir\"\n",
    "\n",
    "        paths = get_pairs_from_paths(inp_images_dir, annotations_dir)\n",
    "        paths = list(zip(*paths))\n",
    "        inp_images = list(paths[0])\n",
    "        annotations = list(paths[1])\n",
    "\n",
    "    assert type(inp_images) is list\n",
    "    assert type(annotations) is list\n",
    "\n",
    "    tp = np.zeros(model.n_classes)\n",
    "    fp = np.zeros(model.n_classes)\n",
    "    fn = np.zeros(model.n_classes)\n",
    "    n_pixels = np.zeros(model.n_classes)\n",
    "\n",
    "    for inp, ann in tqdm(zip(inp_images, annotations)):\n",
    "        pr = predict(model, inp, read_image_type=read_image_type)\n",
    "        gt = get_segmentation_array(ann, model.n_classes,\n",
    "                                    model.output_width, model.output_height,\n",
    "                                    no_reshape=True, read_image_type=read_image_type)\n",
    "        gt = gt.argmax(-1)\n",
    "        pr = pr.flatten()\n",
    "        gt = gt.flatten()\n",
    "\n",
    "        for cl_i in range(model.n_classes):\n",
    "\n",
    "            tp[cl_i] += np.sum((pr == cl_i) * (gt == cl_i))\n",
    "            fp[cl_i] += np.sum((pr == cl_i) * ((gt != cl_i)))\n",
    "            fn[cl_i] += np.sum((pr != cl_i) * ((gt == cl_i)))\n",
    "            n_pixels[cl_i] += np.sum(gt == cl_i)\n",
    "\n",
    "    cl_wise_score = tp / (tp + fp + fn + 0.000000000001)\n",
    "    n_pixels_norm = n_pixels / np.sum(n_pixels)\n",
    "    frequency_weighted_IU = np.sum(cl_wise_score*n_pixels_norm)\n",
    "    mean_IU = np.mean(cl_wise_score)\n",
    "\n",
    "    return {\n",
    "        \"frequency_weighted_IU\": frequency_weighted_IU,\n",
    "        \"mean_IU\": mean_IU,\n",
    "        \"class_wise_IU\": cl_wise_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20723d2-e72b-4483-abdf-2729eeb395e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938cd63e-4cb1-49da-b082-05730d1f931f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f43c2b4-d00d-486b-ad9d-2d78033619d1",
   "metadata": {},
   "source": [
    "### get_vgg_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d8700e-369c-4ef3-8e61-e84a0bc46f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "\n",
    "IMAGE_ORDERING = 'channels_last' #(batch_size, height, width, channels) - default\n",
    "MERGE_AXIS = -1 # because image_ordering is channel_last\n",
    "    \n",
    "if IMAGE_ORDERING == 'channels_first':\n",
    "    pretrained_url = \"https://github.com/fchollet/deep-learning-models/\" \\\n",
    "                     \"releases/download/v0.1/\" \\\n",
    "                     \"vgg16_weights_th_dim_ordering_th_kernels_notop.h5\"\n",
    "elif IMAGE_ORDERING == 'channels_last':\n",
    "    pretrained_url = \"https://github.com/fchollet/deep-learning-models/\" \\\n",
    "                     \"releases/download/v0.1/\" \\\n",
    "                     \"vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
    "\n",
    "def get_vgg_encoder(input_height=512,  input_width= 512, pretrained='imagenet', channels=3):\n",
    "\n",
    "\n",
    "\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same',\n",
    "               name='block1_conv1', data_format=IMAGE_ORDERING)(img_input)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same',\n",
    "               name='block1_conv2', data_format=IMAGE_ORDERING)(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool',\n",
    "                     data_format=IMAGE_ORDERING)(x)\n",
    "    f1 = x\n",
    "    # Block 2\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same',\n",
    "               name='block2_conv1', data_format=IMAGE_ORDERING)(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same',\n",
    "               name='block2_conv2', data_format=IMAGE_ORDERING)(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool',\n",
    "                     data_format=IMAGE_ORDERING)(x)\n",
    "    f2 = x\n",
    "\n",
    "    # Block 3\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same',\n",
    "               name='block3_conv1', data_format=IMAGE_ORDERING)(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same',\n",
    "               name='block3_conv2', data_format=IMAGE_ORDERING)(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same',\n",
    "               name='block3_conv3', data_format=IMAGE_ORDERING)(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool',\n",
    "                     data_format=IMAGE_ORDERING)(x)\n",
    "    f3 = x\n",
    "\n",
    "    # Block 4\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same',\n",
    "               name='block4_conv1', data_format=IMAGE_ORDERING)(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same',\n",
    "               name='block4_conv2', data_format=IMAGE_ORDERING)(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same',\n",
    "               name='block4_conv3', data_format=IMAGE_ORDERING)(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool',\n",
    "                     data_format=IMAGE_ORDERING)(x)\n",
    "    f4 = x\n",
    "\n",
    "    # Block 5\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same',\n",
    "               name='block5_conv1', data_format=IMAGE_ORDERING)(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same',\n",
    "               name='block5_conv2', data_format=IMAGE_ORDERING)(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same',\n",
    "               name='block5_conv3', data_format=IMAGE_ORDERING)(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool',\n",
    "                     data_format=IMAGE_ORDERING)(x)\n",
    "    f5 = x\n",
    "\n",
    "    if pretrained == 'imagenet':\n",
    "        VGG_Weights_path = keras.utils.get_file(\n",
    "            pretrained_url.split(\"/\")[-1], pretrained_url)\n",
    "        Model(img_input, x).load_weights(VGG_Weights_path, by_name=True, skip_mismatch=True)\n",
    "\n",
    "    return img_input, [f1, f2, f3, f4, f5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf530fc-bd83-4aa2-9189-320b4358143f",
   "metadata": {},
   "source": [
    "## 3 get_segmentation_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b36759-33f3-484a-9163-0bd35221a326",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import MethodType\n",
    "\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "import keras.backend as K\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from ..train import train\n",
    "from ..predict import predict, predict_multiple, evaluate\n",
    "\n",
    "def get_segmentation_model(input, output):\n",
    "\n",
    "    img_input = input\n",
    "    o = output\n",
    "\n",
    "    o_shape = Model(img_input, o).output_shape\n",
    "    i_shape = Model(img_input, o).input_shape\n",
    "\n",
    "    IMAGE_ORDERING = 'channels_last'\n",
    "    output_height = o_shape[1]\n",
    "    output_width = o_shape[2]\n",
    "    input_height = i_shape[1]\n",
    "    input_width = i_shape[2]\n",
    "    n_classes = o_shape[3]\n",
    "    o = (Reshape((output_height*output_width, -1)))(o)\n",
    "\n",
    "    o = (Activation('softmax'))(o)\n",
    "    model = Model(img_input, o)\n",
    "    model.output_width = output_width\n",
    "    model.output_height = output_height\n",
    "    model.n_classes = n_classes\n",
    "    model.input_height = input_height\n",
    "    model.input_width = input_width\n",
    "    model.model_name = \"\"\n",
    "\n",
    "    model.train = MethodType(train, model)\n",
    "    model.predict_segmentation = MethodType(predict, model)\n",
    "    model.predict_multiple = MethodType(predict_multiple, model)\n",
    "    model.evaluate_segmentation = MethodType(evaluate, model)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398e87de-8322-4c87-9366-a52f6c126c1e",
   "metadata": {},
   "source": [
    "### 2 Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976afadd-59fb-4871-95fe-777933d65e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _unet(n_classes, encoder, l1_skip_conn=True, input_height=416,\n",
    "          input_width=608, channels=3):\n",
    "    \n",
    "    IMAGE_ORDERING = 'channels_last' #(batch_size, height, width, channels) - default\n",
    "    MERGE_AXIS = -1 # because image_ordering is channel_last\n",
    "\n",
    "    img_input, levels = encoder(input_height=input_height, input_width=input_width, channels=channels)\n",
    "    [f1, f2, f3, f4, f5] = levels\n",
    "\n",
    "    o = f4\n",
    "\n",
    "    o = (ZeroPadding2D((1, 1), data_format=IMAGE_ORDERING))(o)\n",
    "    o = (Conv2D(512, (3, 3), padding='valid' , activation='relu' , data_format=IMAGE_ORDERING))(o)\n",
    "    o = (BatchNormalization())(o)\n",
    "\n",
    "    o = (UpSampling2D((2, 2), data_format=IMAGE_ORDERING))(o)\n",
    "    o = (concatenate([o, f3], axis=MERGE_AXIS))\n",
    "    o = (ZeroPadding2D((1, 1), data_format=IMAGE_ORDERING))(o)\n",
    "    o = (Conv2D(256, (3, 3), padding='valid', activation='relu' , data_format=IMAGE_ORDERING))(o)\n",
    "    o = (BatchNormalization())(o)\n",
    "\n",
    "    o = (UpSampling2D((2, 2), data_format=IMAGE_ORDERING))(o)\n",
    "    o = (concatenate([o, f2], axis=MERGE_AXIS))\n",
    "    o = (ZeroPadding2D((1, 1), data_format=IMAGE_ORDERING))(o)\n",
    "    o = (Conv2D(128, (3, 3), padding='valid' , activation='relu' , data_format=IMAGE_ORDERING))(o)\n",
    "    o = (BatchNormalization())(o)\n",
    "\n",
    "    o = (UpSampling2D((2, 2), data_format=IMAGE_ORDERING))(o)\n",
    "\n",
    "    if l1_skip_conn:\n",
    "        o = (concatenate([o, f1], axis=MERGE_AXIS))\n",
    "\n",
    "    o = (ZeroPadding2D((1, 1), data_format=IMAGE_ORDERING))(o)\n",
    "    o = (Conv2D(64, (3, 3), padding='valid', activation='relu', data_format=IMAGE_ORDERING, name=\"seg_feats\"))(o)\n",
    "    o = (BatchNormalization())(o)\n",
    "\n",
    "    o = Conv2D(n_classes, (3, 3), padding='same',\n",
    "               data_format=IMAGE_ORDERING)(o)\n",
    "\n",
    "    model = get_segmentation_model(img_input, o)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1c1bd7-dbd7-4482-8fc4-2b099c4b5112",
   "metadata": {},
   "source": [
    "### 1 vgg_unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746fe392-cc69-4a38-b175-0e1383ffdc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_unet(n_classes, input_height, input_width, encoder_level=3, channels=3):\n",
    "\n",
    "    model = _unet(n_classes, get_vgg_encoder,\n",
    "                  input_height=input_height, input_width=input_width, channels=channels)\n",
    "    model.model_name = \"vgg_unet\"\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ea917e-b0d5-444e-b46c-8f3e8fb25807",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e521b7e9-f82e-486d-a8dc-32602329709f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd6059f7-e488-4df8-ba3e-dc88259c0051",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e40c05f4-47d8-4a2d-a44f-922c0643177d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f78c1bd-8883-48f0-87e4-91f8bf16fcb9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14e0c013-6792-451a-ae7d-7368f6055e84",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-bc5033b7b6e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvgg_unet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m7\u001b[0m \u001b[1;33m,\u001b[0m  \u001b[0minput_height\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m612\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_width\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow_Deep_Learning\\lib\\site-packages\\keras_segmentation\\models\\unet.py\u001b[0m in \u001b[0;36mvgg_unet\u001b[1;34m(n_classes, input_height, input_width, encoder_level)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m     model = _unet(n_classes, get_vgg_encoder,\n\u001b[1;32m--> 122\u001b[1;33m                   input_height=input_height, input_width=input_width)\n\u001b[0m\u001b[0;32m    123\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"vgg_unet\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow_Deep_Learning\\lib\\site-packages\\keras_segmentation\\models\\unet.py\u001b[0m in \u001b[0;36m_unet\u001b[1;34m(n_classes, encoder, l1_skip_conn, input_height, input_width)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m     img_input, levels = encoder(\n\u001b[1;32m---> 73\u001b[1;33m         input_height=input_height, input_width=input_width)\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;33m[\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf5\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlevels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow_Deep_Learning\\lib\\site-packages\\keras_segmentation\\models\\vgg16.py\u001b[0m in \u001b[0;36mget_vgg_encoder\u001b[1;34m(input_height, input_width, pretrained)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_vgg_encoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_height\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m224\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0minput_width\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m224\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'imagenet'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[1;32massert\u001b[0m \u001b[0minput_height\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m32\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0minput_width\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m32\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = vgg_unet(n_classes=7 ,  input_height=612, input_width=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ce569a-e9be-4922-8495-0fd6b167268f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(\n",
    "    train_images =  \"Cropped_same_name/train/Images/\",\n",
    "    train_annotations = \"Cropped_same_name/train/Masks/\",\n",
    "    checkpoints_path = \"Cropped_same_name/Model_Save/Trial_1/\" , epochs=5  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f546b93-a9f4-4f85-80a3-620cfbb22266",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.predict_segmentation(\n",
    "    inp=\"dataset1/images_prepped_test/0016E5_07965.png\",\n",
    "    out_fname=\"/tmp/out.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2480fe8b-c542-49a3-9041-7cce8313d4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = model.predict_segmentation(\n",
    "    inp=\"dataset1/images_prepped_test/0016E5_07965.png\",\n",
    "    out_fname=\"/tmp/out.png\" , overlay_img=True, show_legends=True,\n",
    "    class_names = [ \"Sky\",    \"Building\", \"Pole\",\"Road\",\"Pavement\",\"Tree\",\"SignSymbol\", \"Fence\", \"Car\",\"Pedestrian\", \"Bicyclist\"]\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3438af1a-574e-452b-92df-147465bdeebd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
